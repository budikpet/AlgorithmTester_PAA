{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of output files\n",
    "## Prepare environment, functions etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas\n",
    "import matplotlib\n",
    "import numpy\n",
    "from algorithm_tester.helpers import FilePair\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Zapnout zobrazování grafů (procento uvozuje „magickou” zkratku IPythonu):\n",
    "%matplotlib inline\n",
    "\n",
    "path = 'tester_results'\n",
    "evo_path = f'{path}/EvoFile'\n",
    "solutions = '../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pandas.set_option('display.max_rows', None)\n",
    "#pandas.read_csv?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_paths_from_dir(path: str, include_sol: bool = False, include_instance: bool = True) -> (str, str, str):\n",
    "    dataset_prefix: str = path.split(\"/\")[-1]\n",
    "    for root, _, files in os.walk(path):\n",
    "        dataset: str = dataset_prefix + \"_\" + \"_\".join(root.replace(path, \"\")[1:].split('/'))\n",
    "        for file in files:\n",
    "            if \"column\" not in file and \".dat\" in file:\n",
    "                if (\"_sol\" in file and include_sol) or (\"_inst\" in file and include_instance):\n",
    "                    yield (dataset, root, file)\n",
    "\n",
    "def get_cols_list(path: str):\n",
    "    cols = pandas.read_csv(path, index_col=None, delimiter=\" \", header=None)\n",
    "    return list(cols.iloc[0])\n",
    "\n",
    "def load_sol_from_dir(folder_path: str, column_list, special_column_name: str = \"instance_info\"):\n",
    "    \"\"\" Load solutions from directory files into table. \"\"\"\n",
    "    it = get_file_paths_from_dir(folder_path, include_sol=True, include_instance=False)\n",
    "    output_table = None\n",
    "        \n",
    "    for (dataset, root, file) in it:\n",
    "        curr_table = pandas.read_csv(f'{root}/{file}', index_col=None, delimiter=\" \", header=None).iloc[:,0:3]\n",
    "        curr_table.columns = column_list\n",
    "        curr_table[\"dataset\"] = dataset\n",
    "        curr_table[special_column_name] = file.split(\"_\")[1]\n",
    "                \n",
    "        if output_table is not None:\n",
    "            output_table = output_table.append(curr_table, ignore_index=True)\n",
    "        else:\n",
    "            output_table = curr_table\n",
    "    \n",
    "    return output_table\n",
    "\n",
    "def load_data_from_dir(folder_path: str, column_list, special_column_name: str = \"instance_info\"):\n",
    "    \"\"\" Load data from directory files into table. \"\"\"\n",
    "    it = get_file_paths_from_dir(folder_path)\n",
    "    output_table = None\n",
    "    \n",
    "    for (dataset, root, file) in it:\n",
    "        curr_table = pandas.read_csv(f'{root}/{file}', index_col=None, delimiter=\" \", header=None)\n",
    "        curr_table.columns = column_list\n",
    "        curr_table[\"dataset\"] = dataset\n",
    "        curr_table[special_column_name] = file.split(\"_\")[1]\n",
    "                \n",
    "        if output_table is not None:\n",
    "            output_table = output_table.append(curr_table, ignore_index=True)\n",
    "        else:\n",
    "            output_table = curr_table\n",
    "    \n",
    "    #output_table = output_table.set_index(['algorithm', 'dataset', 'id', \"item_count\"])\n",
    "    #output_table.sort_values(by=[\"algorithm\", \"dataset\", \"item_count\", \"id\"], inplace=True)\n",
    "    return output_table\n",
    "\n",
    "def construct_table_from(filePair: FilePair):\n",
    "    solution_table = pandas.read_csv(filePair.solutionFile, header=None, index_col=None, delimiter=\" \")\n",
    "    data_table = pandas.read_csv(filePair.dataFile, header=None, index_col=None, delimiter=\" \")\n",
    "    \n",
    "    item_count = data_table.iloc[0, 1]\n",
    "    \n",
    "    solution_table = solution_table.drop_duplicates(subset=[0], keep='first').reset_index()\n",
    "\n",
    "    data_table = data_table.iloc[:, 4:]\n",
    "    data_table = data_table[data_table.columns[::2]]\n",
    "\n",
    "    info_table = pandas.concat([solution_table.iloc[:, 1], solution_table.iloc[:, 3], data_table.max(axis=1)], axis=1)\n",
    "    info_table.columns = [\"id\", \"best_value\", \"max_cost\"]\n",
    "    info_table[\"item_count\"] = item_count\n",
    "    return info_table\n",
    "\n",
    "def create_avg_time_table(table, name: str, column_name: str = \"item_count\"):\n",
    "    # Create a table of average times according to algorithm and item_count columns\n",
    "    avg_times = table.groupby([\"algorithm_name\", column_name])['elapsed_time'] \\\n",
    "        .mean().reset_index()\n",
    "    avg_times = avg_times.round(2)\n",
    "    \n",
    "    avg_configs = table.groupby([\"algorithm_name\", column_name])['elapsed_configs']\\\n",
    "        .mean().reset_index()\n",
    "    \n",
    "    avg_times = avg_times.merge(avg_configs, on=[\"algorithm_name\", column_name])\n",
    "\n",
    "    # Move all values of algorithm column into separate columns\n",
    "    #avg_times = avg_times.unstack(\"algorithm_name\")\n",
    "    #avg_times.columns = avg_times.columns.droplevel()\n",
    "    avg_times.name = f\"Avg #configs per {column_name}\"\n",
    "    avg_times.sort_values(by=column_name, inplace=True)\n",
    "    #avg_times.fillna(\"-\", inplace=True)\n",
    "\n",
    "    # Save the dataframe to csv\n",
    "    #avg_times.to_excel(f'excel/{name}_avg_times.xlsx', sheet_name=name)\n",
    "    \n",
    "    return avg_times\n",
    "\n",
    "def create_avg_error_table(table, column_name: str, table_name: str = \"unknown\"):\n",
    "    table[\"relative_error\"] = numpy.abs(table[\"best_value\"] - table[\"found_value\"])/table[\"best_value\"]\n",
    "    table = table.fillna(0)\n",
    "    \n",
    "    error_group = table.groupby([column_name, \"algorithm_name\"])[\"relative_error\"]\n",
    "\n",
    "    error_max = error_group.max().reset_index() \\\n",
    "        .rename(columns={'relative_error':'max_relative_error'})\n",
    "    error_avg = error_group.mean().reset_index() \\\n",
    "        .rename(columns={'relative_error':'avg_relative_error'})\n",
    "    \n",
    "    # Construct, unstack\n",
    "    avg_error = pandas.merge(error_max, error_avg, on=[column_name, \"algorithm_name\"])\n",
    "    #avg_error = avg_error.set_index([\"algorithm_name\", column_name]).unstack(\"algorithm_name\")\n",
    "    #avg_error = error_max.join(error_avg).round(6)\n",
    "    #avg_error.columns = [\"max_relative_error\", \"avg_relative_error\"]\n",
    "    avg_error.name = f\"Avg & max relative error per {column_name}\"\n",
    "    \n",
    "    \n",
    "    avg_error.set_index([\"algorithm_name\", column_name]).unstack(\"algorithm_name\")\\\n",
    "        .round(6)\\\n",
    "        .to_excel(f\"excel/{table_name}_avg_error.xlsx\", sheet_name=table_name)\n",
    "    \n",
    "    return avg_error\n",
    "\n",
    "def save_plot(table, title: str, column_name: str, output_name: str, y_label: str = \"Relative errors\"):\n",
    "    worktable = table.loc[:, column_name].copy()\n",
    "    \n",
    "    plot = worktable.plot()\n",
    "    plot.set_ylabel(y_label)\n",
    "\n",
    "    figure = plot.get_figure()\n",
    "    figure.suptitle(title)\n",
    "    figure.savefig(f\"excel/{output_name}.pdf\")\n",
    "    \n",
    "    return plot\n",
    "\n",
    "def save_table(table, output_name):\n",
    "    table.round(6).to_excel(f\"excel/{output_name}_table.xlsx\", sheet_name=output_name)\n",
    "    \n",
    "def init_evo_plot(title: str):\n",
    "    fig, axes = plt.subplots(1, 1, sharex=True, sharey=True, figsize=[13,5])\n",
    "    axes.set_xlabel('step')\n",
    "    axes.set_ylabel('Sum of costs')\n",
    "    axes.set_title(title)\n",
    "    return fig, axes\n",
    "    \n",
    "def add_evo_plot(path: str, columns, plot_name: str, axes):\n",
    "    evo_table = pandas.read_csv(path, index_col=None, delimiter=\" \", header=None)\n",
    "    evo_table.columns = evo_cols\n",
    "    evo_table[\"step\"] = range(evo_table['current_temperature'].count())\n",
    "    \n",
    "    data = evo_table.sort_values(by=\"step\").set_index([\"step\"]).loc[:, \"best_cost\"]\n",
    "    \n",
    "    axes.plot(data, label=f\"best_cost_{plot_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column lists\n",
    "\n",
    "sol_cols = [\"id\", \"item_count\", \"best_value\"]\n",
    "cols = get_cols_list(f'{path}/column_description.dat')\n",
    "\n",
    "evo_cols = get_cols_list(f'{evo_path}/column_description.evo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put data from all analysis files into tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tables of all strategies\n",
    "#balance_table = load_data_from_dir(f'{path}/Balance', cols) \\\n",
    "#    .rename(columns={'instance_info':'balance'})\n",
    "#robust_table = load_data_from_dir(f'{path}/Robust', cols) \\\n",
    "#    .drop(columns=\"instance_info\")\n",
    "\n",
    "nk_table = load_data_from_dir(f'{path}/NK', cols) \\\n",
    "    .drop(columns=\"things\") \\\n",
    "    .merge(load_sol_from_dir(f'{solutions}/NK', sol_cols).drop(columns=\"dataset\"), on=[\"id\", \"item_count\"])\n",
    "zkc_table = load_data_from_dir(f'{path}/ZKC', cols) \\\n",
    "    .drop(columns=\"things\") \\\n",
    "    .merge(load_sol_from_dir(f'{solutions}/ZKC', sol_cols).drop(columns=\"dataset\"), on=[\"id\", \"item_count\"])\n",
    "zkw_table = load_data_from_dir(f'{path}/ZKW', cols) \\\n",
    "    .drop(columns=\"things\") \\\n",
    "    .merge(load_sol_from_dir(f'{solutions}/ZKW', sol_cols).drop(columns=\"dataset\"), on=[\"id\", \"item_count\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_nk = create_avg_error_table(nk_table, \"item_count\")\n",
    "error_zkc = create_avg_error_table(zkc_table, \"item_count\")\n",
    "error_zkw = create_avg_error_table(zkw_table, \"item_count\")\n",
    "\n",
    "time_nk = create_avg_time_table(nk_table, name=\"initial temperature\", column_name=\"item_count\")\n",
    "time_zkc = create_avg_time_table(zkc_table, name=\"cooling coefficient\", column_name=\"item_count\")\n",
    "time_zkw = create_avg_time_table(zkw_table, name=\"number of cycles\", column_name=\"item_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_table = nk_table.append(zkc_table).append(zkw_table).query(\"algorithm_name != 'SA_OLD'\")\n",
    "sapenalty_error_comp = create_avg_error_table(full_table, \"item_count\", \"sapenalty_comp\")\\\n",
    "    .set_index([\"item_count\", \"algorithm_name\"]).unstack(\"algorithm_name\")\\\n",
    "    .drop(columns=\"max_relative_error\")\n",
    "\n",
    "sapenalty_comp_plot = sapenalty_error_comp.plot.bar(legend=True)\n",
    "sapenalty_comp_plot.set_ylabel(\"Relative errors\")\n",
    "\n",
    "figure = sapenalty_comp_plot.get_figure()\n",
    "figure.suptitle(\"SA/SAPenalty relative error comparison\")\n",
    "figure.savefig(\"excel/sapenalty_comp.pdf\")\n",
    "\n",
    "sapenalty_comp_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter analysis\n",
    "## Analyze the same dataset, change one of the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_sols = load_sol_from_dir(f'{solutions}/ParamAnalysis', sol_cols) \\\n",
    "    .drop(columns=\"dataset\")\n",
    "\n",
    "init_temp_table = load_data_from_dir(f'{path}/ParamAnalysis/InitTemperature', cols) \\\n",
    "    .drop(columns=[\"things\", \"cycles\", \"min_temperature\", \"cooling\"]) \\\n",
    "    .merge(param_sols, on=[\"id\", \"item_count\"])\n",
    "init_temp_table[\"init_temp\"] = init_temp_table[\"dataset\"]\\\n",
    "    .str.split(\"_\", n = 1, expand = True)[1].astype(int)\n",
    "\n",
    "cooling_table = load_data_from_dir(f'{path}/ParamAnalysis/Cooling', cols) \\\n",
    "    .drop(columns=[\"things\", \"cycles\", \"min_temperature\", \"init_temperature\"]) \\\n",
    "    .merge(param_sols, on=[\"id\", \"item_count\"])\n",
    "cooling_table[\"cooling_coef\"] = cooling_table[\"dataset\"]\\\n",
    "    .str.split(\"_\", n = 1, expand = True)[1].str.replace(\",\", \".\").astype(float)\n",
    "\n",
    "cycles_table = load_data_from_dir(f'{path}/ParamAnalysis/Cycles', cols) \\\n",
    "    .drop(columns=[\"things\", \"init_temperature\", \"min_temperature\", \"cooling\"]) \\\n",
    "    .merge(param_sols, on=[\"id\", \"item_count\"])\n",
    "cycles_table[\"cycles\"] = cycles_table[\"dataset\"]\\\n",
    "    .str.split(\"_\", n = 1, expand = True)[1].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_init_temp = create_avg_error_table(init_temp_table, \"init_temp\")\\\n",
    "    .query('algorithm_name == \"SA\"').set_index(\"init_temp\")\n",
    "error_cooling = create_avg_error_table(cooling_table, \"cooling_coef\")\\\n",
    "    .query('algorithm_name == \"SA\"').set_index(\"cooling_coef\")\n",
    "error_cycles = create_avg_error_table(cycles_table, \"cycles\")\\\n",
    "    .query('algorithm_name == \"SA\"').set_index(\"cycles\")\n",
    "\n",
    "time_init_temp = create_avg_time_table(init_temp_table, name=\"initial temperature\", column_name=\"init_temp\")\\\n",
    "    .query('algorithm_name == \"SA\"').set_index(\"init_temp\")\n",
    "time_cooling = create_avg_time_table(cooling_table, name=\"cooling coefficient\", column_name=\"cooling_coef\")\\\n",
    "    .query('algorithm_name == \"SA\"').set_index(\"cooling_coef\")\n",
    "time_cycles = create_avg_time_table(cycles_table, name=\"number of cycles\", column_name=\"cycles\")\\\n",
    "    .query('algorithm_name == \"SA\"').set_index(\"cycles\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(error_init_temp)\n",
    "\n",
    "out = pandas.merge(error_init_temp, time_init_temp, on=[\"algorithm_name\", \"init_temp\"])\n",
    "save_table(out, \"init_temp_errors_speed\")\n",
    "save_plot(error_init_temp, \"Avg error - init temperatures\", \"avg_relative_error\", \"init_temp_avg_error\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time_init_temp)\n",
    "\n",
    "save_plot(time_init_temp, \"Avg speed - init temperatures\", \"elapsed_time\", \"init_temp_time_ms\", \"Time[ms]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_plot(time_init_temp, \"Avg speed - init temperatures\", \"elapsed_configs\", \"init_temp_time_configs\", \"Time[configs]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = init_evo_plot(\"Evolution of costs for different init temperatures\")\n",
    "\n",
    "add_evo_plot(f\"{evo_path}/InitTemperature/EvoFile_40_inst_SA_1500.evo\", evo_cols, \"1500\", axes)\n",
    "add_evo_plot(f\"{evo_path}/InitTemperature/EvoFile_40_inst_SA_2500.evo\", evo_cols, \"2500\", axes)\n",
    "axes.legend()\n",
    "\n",
    "fig.savefig(f\"excel/init_temp_evo.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(error_cooling)\n",
    "\n",
    "out = pandas.merge(error_cooling, time_cooling, on=[\"algorithm_name\", \"cooling_coef\"])\n",
    "save_table(out, \"cooling_errors_speed\")\n",
    "save_plot(error_cooling, \"Avg error - cooling\", \"avg_relative_error\", \"cooling_avg_error\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_plot(time_cooling, \"Avg speed - cooling\", \"elapsed_time\", \"cooling_time_ms\", \"Time[ms]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_plot(time_cooling, \"Avg speed - cooling\", \"elapsed_configs\", \"cooling_time_configs\", \"Time[configs]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = init_evo_plot(\"Evolution of costs for different cooling\")\n",
    "\n",
    "add_evo_plot(f\"{evo_path}/Cooling/EvoFile_40_inst_SA_950.evo\", evo_cols, \"950\", axes)\n",
    "add_evo_plot(f\"{evo_path}/Cooling/EvoFile_40_inst_SA_970.evo\", evo_cols, \"970\", axes)\n",
    "\n",
    "axes.legend()\n",
    "\n",
    "fig.savefig(f\"excel/cooling_evo.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(error_cycles)\n",
    "\n",
    "out = pandas.merge(error_cycles, time_cycles, on=[\"algorithm_name\", \"cycles\"])\n",
    "save_table(out, \"cycles_errors_speed\")\n",
    "save_plot(error_cycles, \"Avg error - cycles\", \"avg_relative_error\", \"cycles_avg_error\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_plot(time_cycles, \"Avg speed - cycles\", \"elapsed_time\", \"cycles_time_configs\", \"Time[ms]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_plot(time_cycles, \"Avg speed - cycles\", \"elapsed_configs\", \"cycles_time_ms\", \"Time[configs]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = init_evo_plot(\"Evolution of costs for different cycles\")\n",
    "\n",
    "add_evo_plot(f\"{evo_path}/Cycles/EvoFile_40_inst_SA_75.evo\", evo_cols, \"75\", axes)\n",
    "add_evo_plot(f\"{evo_path}/Cycles/EvoFile_40_inst_SA_100.evo\", evo_cols, \"100\", axes)\n",
    "\n",
    "axes.legend()\n",
    "\n",
    "fig.savefig(f\"excel/cycles_evo.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataAnalysis\n",
    "## Analyze datasets that were generated differently with fixed SA parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpath = f'{path}/DataAnalysis'\n",
    "dsolutions = f'{solutions}/DataAnalysis'\n",
    "\n",
    "balance_table = load_data_from_dir(f'{dpath}/Balance', cols) \\\n",
    "    .merge(load_sol_from_dir(f'{dsolutions}/Balance', sol_cols).drop(columns=\"dataset\"), on=[\"id\", \"instance_info\"])\\\n",
    "    .rename(columns={'instance_info':'balance'})\n",
    "correlation_table = load_data_from_dir(f'{dpath}/Correlation', cols) \\\n",
    "    .merge(load_sol_from_dir(f'{dsolutions}/Correlation', sol_cols).drop(columns=\"dataset\"), on=[\"id\", \"instance_info\"])\\\n",
    "    .rename(columns={'instance_info':'correlation'})\n",
    "granularity_heavy_table = load_data_from_dir(f'{dpath}/GranularityHeavy', cols) \\\n",
    "    .merge(load_sol_from_dir(f'{dsolutions}/GranularityHeavy', sol_cols).drop(columns=\"dataset\"), on=[\"id\", \"instance_info\"])\\\n",
    "    .rename(columns={'instance_info':'constant'})\n",
    "granularity_light_table = load_data_from_dir(f'{dpath}/GranularityLight', cols) \\\n",
    "    .merge(load_sol_from_dir(f'{dsolutions}/GranularityLight', sol_cols).drop(columns=\"dataset\"), on=[\"id\", \"instance_info\"])\\\n",
    "    .rename(columns={'instance_info':'constant'})\n",
    "maxcost_table = load_data_from_dir(f'{dpath}/MaxCost', cols) \\\n",
    "    .merge(load_sol_from_dir(f'{dsolutions}/MaxCost', sol_cols).drop(columns=\"dataset\"), on=[\"id\", \"instance_info\"])\\\n",
    "    .rename(columns={'instance_info':'maxcost'})\n",
    "maxweight_table = load_data_from_dir(f'{dpath}/MaxWeight', cols) \\\n",
    "    .merge(load_sol_from_dir(f'{dsolutions}/MaxWeight', sol_cols).drop(columns=\"dataset\"), on=[\"id\", \"instance_info\"])\\\n",
    "    .rename(columns={'instance_info':'maxweight'})\n",
    "things_table = load_data_from_dir(f'{dpath}/Things', cols)\n",
    "weight_cap_ratio_table = load_data_from_dir(f'{dpath}/WeightCapRation', cols) \\\n",
    "    .merge(load_sol_from_dir(f'{dsolutions}/WeightCapRation', sol_cols).drop(columns=\"dataset\"), on=[\"id\", \"instance_info\"])\\\n",
    "    .rename(columns={'instance_info':'ratio'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_balance = create_avg_error_table(balance_table, \"balance\")\n",
    "error_corr = create_avg_error_table(correlation_table, \"correlation\")\n",
    "error_granularity_heavy = create_avg_error_table(granularity_heavy_table, \"constant\")\n",
    "error_granularity_light = create_avg_error_table(granularity_light_table, \"constant\")\n",
    "error_maxcost = create_avg_error_table(maxcost_table, \"maxcost\")\n",
    "error_maxweight = create_avg_error_table(maxweight_table, \"maxweight\")\n",
    "error_weightcap = create_avg_error_table(weight_cap_ratio_table, \"ratio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_avg_error_table(, \"maxcost\")\n",
    "print(error_balance.query('algorithm_name == \"SA\"'))\n",
    "error_balance.query('algorithm_name == \"SA\"').set_index(\"balance\").loc[:, \"avg_relative_error\"].plot.bar(legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(error_corr.query('algorithm_name == \"SA\"'))\n",
    "error_corr.query('algorithm_name == \"SA\"').set_index(\"correlation\").loc[:, \"avg_relative_error\"].plot.bar(legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(error_granularity_heavy.query('algorithm_name == \"SA\"'))\n",
    "error_granularity_heavy.query('algorithm_name == \"SA\"').set_index(\"constant\").loc[:, \"avg_relative_error\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(error_granularity_light.query('algorithm_name == \"SA\"'))\n",
    "error_granularity_light.query('algorithm_name == \"SA\"').set_index(\"constant\").loc[:, \"avg_relative_error\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(error_maxcost.query('algorithm_name == \"SA\"'))\n",
    "error_maxcost.query('algorithm_name == \"SA\"').set_index(\"maxcost\").loc[:, \"avg_relative_error\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(error_maxweight.query('algorithm_name == \"SA\"'))\n",
    "error_maxweight.query('algorithm_name == \"SA\"').set_index(\"maxweight\").loc[:, \"avg_relative_error\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(error_weightcap.query('algorithm_name == \"SA\"'))\n",
    "error_weightcap.query('algorithm_name == \"SA\"').set_index(\"ratio\").loc[:, \"avg_relative_error\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
